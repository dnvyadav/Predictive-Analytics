{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nv/Downloads\n"
     ]
    }
   ],
   "source": [
    "cd /home/nv/Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(50000,)\n",
      "(50000,)\n",
      "Classes: \n",
      "[0 1]\n",
      "Number of words: \n",
      "88585\n",
      "Review length: \n",
      "Mean 234.76 words (172.911495)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAFkCAYAAACAUFlOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwOYTSe9HAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7712f4c990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and Plot the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot\n",
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "X = numpy.concatenate((X_train, X_test), axis=0)\n",
    "y = numpy.concatenate((y_train, y_test), axis=0)\n",
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(numpy.unique(y))\n",
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(numpy.unique(numpy.hstack(X))))\n",
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = map(len, X)\n",
    "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\n",
    "# plot review length as a boxplot and histogram\n",
    "pyplot.subplot(121)\n",
    "pyplot.boxplot(result)\n",
    "pyplot.subplot(122)\n",
    "pyplot.hist(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 500, 32)       160000      embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 16000)         0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 250)           4000250     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             251         dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 4160501\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 55s - loss: 0.5366 - acc: 0.6864 - val_loss: 0.2966 - val_acc: 0.8758\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 56s - loss: 0.2048 - acc: 0.9208 - val_loss: 0.2982 - val_acc: 0.8764\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 58s - loss: 0.0725 - acc: 0.9794 - val_loss: 0.3935 - val_acc: 0.8655\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 66s - loss: 0.0153 - acc: 0.9968 - val_loss: 0.4752 - val_acc: 0.8633\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 55s - loss: 0.0025 - acc: 0.9999 - val_loss: 0.5428 - val_acc: 0.8669\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 57s - loss: 7.3811e-04 - acc: 1.0000 - val_loss: 0.5797 - val_acc: 0.8674\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 64s - loss: 4.1734e-04 - acc: 1.0000 - val_loss: 0.6054 - val_acc: 0.8681\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 65s - loss: 2.7427e-04 - acc: 1.0000 - val_loss: 0.6274 - val_acc: 0.8681\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 65s - loss: 1.9458e-04 - acc: 1.0000 - val_loss: 0.6472 - val_acc: 0.8684\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 65s - loss: 1.4375e-04 - acc: 1.0000 - val_loss: 0.6633 - val_acc: 0.8684\n",
      "Accuracy: 86.84%\n"
     ]
    }
   ],
   "source": [
    "# MLP for the IMDB problem, optimizer='adam', nb_epoch =10, input_length(max_words)=500\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10, batch_size=128,verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this model overfits very quicky so i will change it very few training epochs and sincce there is a lot of data so i will keep batch size same as 128   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 500, 32)       160000      embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 16000)         0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 250)           4000250     flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             251         dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 4160501\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 58s - loss: 0.5366 - acc: 0.6864 - val_loss: 0.2966 - val_acc: 0.8758\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 58s - loss: 0.2048 - acc: 0.9208 - val_loss: 0.2982 - val_acc: 0.8764\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 55s - loss: 0.0725 - acc: 0.9794 - val_loss: 0.3935 - val_acc: 0.8655\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 48s - loss: 0.0153 - acc: 0.9968 - val_loss: 0.4752 - val_acc: 0.8633\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 61s - loss: 0.0025 - acc: 0.9999 - val_loss: 0.5428 - val_acc: 0.8669\n",
      "Accuracy: 86.69%\n"
     ]
    }
   ],
   "source": [
    "# MLP for the IMDB problem,optimizer='adam',input_length (max_words)=500, nb_epoch=5\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=5, batch_size=128,verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 1000, 32)      160000      embedding_input_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 32000)         0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 250)           8000250     flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 1)             251         dense_5[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 8160501\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 109s - loss: 0.4993 - acc: 0.7350 - val_loss: 0.3011 - val_acc: 0.8716\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 105s - loss: 0.1888 - acc: 0.9294 - val_loss: 0.2982 - val_acc: 0.8744\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 103s - loss: 0.0619 - acc: 0.9826 - val_loss: 0.3901 - val_acc: 0.8668\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 130s - loss: 0.0099 - acc: 0.9985 - val_loss: 0.4957 - val_acc: 0.8660\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 109s - loss: 0.0019 - acc: 0.9999 - val_loss: 0.5418 - val_acc: 0.8678\n",
      "Accuracy: 86.78%\n"
     ]
    }
   ],
   "source": [
    "# MLP for the IMDB problem for input length 1000 (max_words),optimizer='adam',nb_epoch = 5\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "max_words = 1000\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=5, batch_size=128,verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_9 (Embedding)          (None, 500, 32)       160000      embedding_input_9[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)              (None, 16000)         0           embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 250)           4000250     flatten_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 1)             251         dense_17[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 4160501\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 60s - loss: 0.4978 - acc: 0.7324 - val_loss: 0.3045 - val_acc: 0.8711\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 45s - loss: 0.1985 - acc: 0.9220 - val_loss: 0.3105 - val_acc: 0.8723\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 45s - loss: 0.0791 - acc: 0.9730 - val_loss: 0.4628 - val_acc: 0.8523\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 44s - loss: 0.0233 - acc: 0.9928 - val_loss: 0.5901 - val_acc: 0.8502\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 44s - loss: 0.0063 - acc: 0.9983 - val_loss: 0.7891 - val_acc: 0.8426\n",
      "Accuracy: 84.26%\n"
     ]
    }
   ],
   "source": [
    "# MLP for the IMDB problem for input length 500 (max_words),optimizer='rmsprop',nb_epoch = 5\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=5, batch_size=128,verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_10 (Embedding)         (None, 1000, 32)      160000      embedding_input_10[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)             (None, 32000)         0           embedding_10[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 250)           8000250     flatten_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 1)             251         dense_19[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 8160501\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 102s - loss: 0.5048 - acc: 0.7457 - val_loss: 0.3070 - val_acc: 0.8697\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 102s - loss: 0.2030 - acc: 0.9189 - val_loss: 0.3002 - val_acc: 0.8754\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 97s - loss: 0.0939 - acc: 0.9656 - val_loss: 0.3962 - val_acc: 0.8658\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 103s - loss: 0.0325 - acc: 0.9892 - val_loss: 0.5644 - val_acc: 0.8560\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 98s - loss: 0.0087 - acc: 0.9971 - val_loss: 0.6809 - val_acc: 0.8546\n",
      "Accuracy: 85.46%\n"
     ]
    }
   ],
   "source": [
    "# MLP for the IMDB problem for input length 1000 (max_words),optimizer='rmsprop',nb_epoch = 5\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "max_words = 1000\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=5, batch_size=128,verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nv/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode='max')\n",
      "/home/nv/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  mode='max')\n",
      "/home/nv/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  mode='max')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (None, 500, 32)       160000      embedding_input_4[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 500, 32)       3104        embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 250, 32)       0           convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 8000)          0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 250)           2000250     flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             251         dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2163605\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 74s - loss: 0.4857 - acc: 0.7164 - val_loss: 0.2764 - val_acc: 0.8864\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 74s - loss: 0.2249 - acc: 0.9123 - val_loss: 0.2675 - val_acc: 0.8890\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 74s - loss: 0.1680 - acc: 0.9359 - val_loss: 0.2855 - val_acc: 0.8841\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 120s - loss: 0.1242 - acc: 0.9554 - val_loss: 0.3293 - val_acc: 0.8810\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 125s - loss: 0.0825 - acc: 0.9729 - val_loss: 0.3956 - val_acc: 0.8749\n",
      "Accuracy: 87.49%\n"
     ]
    }
   ],
   "source": [
    "# CNN for the IMDB problem with input size =max_words 500,optimizer='adam', nb_epoch=5\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "# pad dataset to a maximum review length in words\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same',\n",
    "activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=5, batch_size=128,\n",
    "verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_8 (Embedding)          (None, 1000, 32)      160000      embedding_input_8[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_5 (Convolution1D)  (None, 1000, 32)      3104        embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_5 (MaxPooling1D)    (None, 500, 32)       0           convolution1d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)              (None, 16000)         0           maxpooling1d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 250)           4000250     flatten_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 1)             251         dense_15[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 4163605\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 156s - loss: 0.4529 - acc: 0.7555 - val_loss: 0.3227 - val_acc: 0.8612\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 186s - loss: 0.2360 - acc: 0.9060 - val_loss: 0.2768 - val_acc: 0.8850\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 170s - loss: 0.1769 - acc: 0.9329 - val_loss: 0.3257 - val_acc: 0.8713\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 212s - loss: 0.1306 - acc: 0.9521 - val_loss: 0.3263 - val_acc: 0.8785\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 209s - loss: 0.0845 - acc: 0.9724 - val_loss: 0.3977 - val_acc: 0.8707\n",
      "Accuracy: 87.07%\n"
     ]
    }
   ],
   "source": [
    "# CNN for the IMDB problem with input size =max_words 1000,optimizer='adam', nb_epoch = 5\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "# pad dataset to a maximum review length in words\n",
    "max_words = 1000\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same',\n",
    "activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=5, batch_size=128,\n",
    "verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_6 (Embedding)          (None, 500, 32)       160000      embedding_input_6[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_3 (Convolution1D)  (None, 500, 32)       3104        embedding_6[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_3 (MaxPooling1D)    (None, 250, 32)       0           convolution1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 8000)          0           maxpooling1d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 250)           2000250     flatten_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 1)             251         dense_11[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 2163605\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 100s - loss: 0.4547 - acc: 0.7606 - val_loss: 0.3823 - val_acc: 0.8306\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 98s - loss: 0.2494 - acc: 0.8985 - val_loss: 0.2613 - val_acc: 0.8920\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 98s - loss: 0.2039 - acc: 0.9195 - val_loss: 0.2836 - val_acc: 0.8806\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 158s - loss: 0.1755 - acc: 0.9320 - val_loss: 0.2996 - val_acc: 0.8814\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 109s - loss: 0.1482 - acc: 0.9443 - val_loss: 0.3039 - val_acc: 0.8852\n",
      "Accuracy: 88.52%\n"
     ]
    }
   ],
   "source": [
    "# CNN for the IMDB problem with input size =max_words 500 , optimizer='rmsprop',nb_epoch =5\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "# pad dataset to a maximum review length in words\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same',\n",
    "activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=5, batch_size=128,\n",
    "verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_7 (Embedding)          (None, 1000, 32)      160000      embedding_input_7[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_4 (Convolution1D)  (None, 1000, 32)      3104        embedding_7[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_4 (MaxPooling1D)    (None, 500, 32)       0           convolution1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)              (None, 16000)         0           maxpooling1d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 250)           4000250     flatten_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 1)             251         dense_13[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 4163605\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 194s - loss: 0.4642 - acc: 0.7551 - val_loss: 0.4727 - val_acc: 0.7914\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 197s - loss: 0.2475 - acc: 0.9001 - val_loss: 0.2583 - val_acc: 0.8916\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 186s - loss: 0.1992 - acc: 0.9233 - val_loss: 0.3620 - val_acc: 0.8576\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 193s - loss: 0.1725 - acc: 0.9344 - val_loss: 0.2779 - val_acc: 0.8884\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 221s - loss: 0.1475 - acc: 0.9448 - val_loss: 0.3049 - val_acc: 0.8840\n",
      "Accuracy: 88.40%\n"
     ]
    }
   ],
   "source": [
    "# CNN for the IMDB problem with input size =max_words 1000,optimizer='rmsprop'\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "# pad dataset to a maximum review length in words\n",
    "max_words = 1000\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same',\n",
    "activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=5, batch_size=128,\n",
    "verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
